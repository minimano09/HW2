{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2: Discovery of Frequent Itemsets and Association Rules"
      ],
      "metadata": {
        "id": "9PUfOvTQ2frB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This homework has been done by Group 9, the students are Anna Kov√°cs and Alex Orlandi."
      ],
      "metadata": {
        "id": "_3rSCyri2nD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task is to implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions, and develop and implement an algorithm for generating association rules between frequent itemsets discovered using the A-Priori algorithm. We wrote the code on this Google Colab."
      ],
      "metadata": {
        "id": "Jf1pf1Wp2n6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "6C14vGKD3TaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we set up the environment for performing distributed data processing with PySpark. We set up Spark to run locally within Google Colab."
      ],
      "metadata": {
        "id": "5ZBhUDoJ3hZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE8SZKmK72O-",
        "outputId": "4dc110d1-f57b-4657-a9c4-d8fdb8ae5e37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W5gb-RxF10-c"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf # Spark's core classes\n",
        "\n",
        "import findspark # To set up the environment for pyspark\n",
        "\n",
        "from google.colab import files\n",
        "from itertools import combinations\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init() # findspark's initialization\n",
        "\n",
        "conf = SparkConf().setAppName(\"Homework_2_frequent_itemsets\").setMaster(\"local[*]\") # Local mode\n",
        "sc = SparkContext(conf=conf) # Entry point for Spark's functionalities"
      ],
      "metadata": {
        "id": "hpyoI-OE36O7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "rKZzej5_4Mvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the dataset called \"transactions.txt\", we created it in order to easily visualize the correct functioning of our code. It's a sales transactions dataset, where the number of rows is 13. The dataset is available here:\n",
        "https://drive.google.com/file/d/1UCqxACBmXkd-VRf0v55MMOGn6KBbV1EW/view?usp=sharing"
      ],
      "metadata": {
        "id": "BJwXU8jF4OST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "file_name = \"transactions.txt\"\n",
        "transactions = sc.textFile(\"file:\" + \"/content/\" + file_name).map(lambda line: line.strip().split(\" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "yai7ZYnb4VgA",
        "outputId": "236bc3ca-a18d-4a95-92ff-e8950edf2e14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd8ab998-8562-4d55-87af-bf789260500b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cd8ab998-8562-4d55-87af-bf789260500b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving transactions.txt to transactions (2).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 10 rows:\\n\")\n",
        "first_10_lines = transactions.take(10)\n",
        "for i, line in enumerate(first_10_lines, 1):\n",
        "    print(f\"{i}: {line}\")\n",
        "\n",
        "total_lines = transactions.count()\n",
        "print(f\"\\nTotal number of lines: {total_lines}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9muQw9__1Lw",
        "outputId": "11d13eeb-6221-4097-9c47-c577d06f63ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 rows:\n",
            "\n",
            "1: ['1', '2', '3', '4']\n",
            "2: ['1', '3']\n",
            "3: ['1', '4', '5', '6']\n",
            "4: ['2', '3', '4', '5']\n",
            "5: ['2', '3', '4']\n",
            "6: ['5', '2']\n",
            "7: ['5', '1']\n",
            "8: ['1', '4', '5']\n",
            "9: ['1']\n",
            "10: ['1', '2']\n",
            "\n",
            "Total number of lines: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "f7u7jxKjAyjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set up the parameters that we will use from now on. These include s, which is the support threshold (the minimum fraction of transactions in which an itemset must appear to be considered frequent). We used 0.2, meaning that an itemset must appear in at least 20% of the transactions, because we thought it is a reasonable percentage, not too low (almost all itemsets included) and not too high (no itemsets included). Then we defined number_of_transactions (the total amount of transactions), frequency_threshold (the minimum support count for an itemset to be considered frequent), and the minimum confidence threshold, that we will use later for creating the associations rules. We chose 0.6 because we thought it balances the trade-off between finding meaningful rules and not being too strict."
      ],
      "metadata": {
        "id": "Lr-fPDN_A1UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0.2 # Support threshold\n",
        "number_of_transactions = transactions.count() # Total count of transactions\n",
        "frequency_threshold = s * number_of_transactions # Minimum support count for an itemset to be considered frequent\n",
        "c = 0.6 # Minimum confidence threshold (used for creating the associations rules)\n",
        "\n",
        "print(frequency_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZQJ8fcx8l9v",
        "outputId": "29a9213a-f582-463a-b137-919a48866a29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A-Priori Algorithm"
      ],
      "metadata": {
        "id": "2J0ItnioB-_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core idea of the A-Priori Algorithm: if an itemset is frequent, all its subsets must also be frequent. On the other hand, if an itemset is infrequent, all its supersets are guaranteed to be infrequent.\n",
        "\n",
        "The algorithm consists of 5 steps:\n",
        "\n",
        "1) Find frequent 1-itemsets\n",
        "\n",
        "2) Generate candidate k-itemsets\n",
        "\n",
        "3) Prune candidates\n",
        "\n",
        "4) Count support for candidates\n",
        "\n",
        "5) Iterate"
      ],
      "metadata": {
        "id": "zAqAHBKoCBvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(previous_freq_itemsets, freq_1_itemsets, k):\n",
        "    \"\"\"\n",
        "\n",
        "    In this function we generate k-itemset candidates by pairing\n",
        "    (k-1)-itemsets with 1-itemsets. For each (k-1)-itemset, we try\n",
        "    to add each 1-itemset, creating a new candidate if it has exaclty\n",
        "    k items.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Previous freq itemsets: {previous_freq_itemsets}\")\n",
        "    print(f\"\\n1 freq itemsets: {freq_1_itemsets}\")\n",
        "    print(f\"\\nGenerate candidates for k={k}\" )\n",
        "\n",
        "    candidates = set()\n",
        "\n",
        "    for itemset in previous_freq_itemsets:\n",
        "        for item in freq_1_itemsets:\n",
        "            # Create a new candidate by adding the 1-itemset to the (k-1)-itemset\n",
        "            candidate = itemset | item\n",
        "\n",
        "            # Only add if the resulting candidate has exactly k items\n",
        "            if len(candidate) == k:\n",
        "                candidates.add(candidate)\n",
        "\n",
        "    print(candidates)\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def prune_candidates(candidates, previous_freq_itemsets):\n",
        "    \"\"\"\n",
        "\n",
        "    Here we filter out candidates that contain any infrequent subset.\n",
        "    We iterate through each candidate itemset and check if every subset\n",
        "    of size k-1 is infrequent. If any subset is missing, the candidate\n",
        "    is pruned.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    pruned_candidates = set()\n",
        "\n",
        "    for candidate in candidates:\n",
        "        is_valid = True\n",
        "\n",
        "        # Generate all possible (k-1)-itemsets by removing one item at a time\n",
        "        for item in candidate:\n",
        "            subset = candidate - frozenset([item])\n",
        "\n",
        "            # Check if the subset is in the frequent (k-1)-itemsets\n",
        "            if subset not in previous_freq_itemsets:\n",
        "                is_valid = False\n",
        "                break\n",
        "\n",
        "        # If all (k-1)-subsets are frequent, add candidate to the pruned set\n",
        "        if is_valid:\n",
        "            pruned_candidates.add(candidate)\n",
        "\n",
        "    print(f\"\\nPruned candidates: {pruned_candidates}\")\n",
        "\n",
        "    return pruned_candidates"
      ],
      "metadata": {
        "id": "jNL5mA9FAXaq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_frequent_itemsets = time.time()\n",
        "\n",
        "frequent_itemsets = dict() # Frequent 1-itemsets\n",
        "s_count = dict() # Support counts for frequent itemsets\n",
        "\n",
        "# Flatten items in each transaction into pairs of (item, 1) and\n",
        "# sum the counts, giving the support count of each item\n",
        "item_appear = transactions.flatMap(lambda items: [(item, 1) for item in items]).reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "# Filter itemsets to keep only those with support >= frequency_threshold\n",
        "frequent_1_itemsets_tuples = item_appear.filter(lambda item: item[1] >= frequency_threshold).map(lambda item: (frozenset([item[0]]), item[1])).collect()\n",
        "\n",
        "# frequent_1_itemsets stores frequent 1-itemsets with their counts\n",
        "frequent_1_itemsets = set()\n",
        "\n",
        "for itemset, count in frequent_1_itemsets_tuples:\n",
        "    frequent_itemsets.setdefault(1, set()).add(itemset)\n",
        "    s_count[itemset] = count\n",
        "    frequent_1_itemsets.update(itemset)\n",
        "\n",
        "print(len(frequent_1_itemsets))\n",
        "print(s_count)\n",
        "\n",
        "# Convert individual items in freq_1_itemsets to frozensets (immutable sets)\n",
        "frequent_1_itemsets = set(frozenset([item]) for item in frequent_1_itemsets)\n",
        "\n",
        "print(f\"\\nfreq_1_itemsets: {frequent_1_itemsets}\")"
      ],
      "metadata": {
        "id": "yZzZoH6BFh5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cecd581-7b48-466f-c26b-cbe8919441e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "{frozenset({'1'}): 10, frozenset({'4'}): 5, frozenset({'2'}): 8, frozenset({'3'}): 4, frozenset({'5'}): 5}\n",
            "\n",
            "freq_1_itemsets: {frozenset({'5'}), frozenset({'2'}), frozenset({'4'}), frozenset({'1'}), frozenset({'3'})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=2\n",
        "\n",
        "while True:\n",
        "\n",
        "    # Ensure previous level of frequent itemsets exists for k-1\n",
        "    if k - 1 not in frequent_itemsets or len(frequent_itemsets[k-1]) == 0:\n",
        "        break\n",
        "\n",
        "    # k-itemset candidates are generated from the (k-1)-itemsets\n",
        "    candidates_k = generate_candidates(frequent_itemsets[k-1], frequent_1_itemsets, k)\n",
        "    if not candidates_k:\n",
        "        break\n",
        "\n",
        "    # Candidates with infrequent subsets are removed\n",
        "    candidates_k = prune_candidates(candidates_k, frequent_itemsets[k-1])\n",
        "    if not candidates_k:\n",
        "        break\n",
        "\n",
        "    # Generate all k-itemsets (combinations of size k) from each transaction as frozensets, then pair each with 1 for counting\n",
        "    transaction_k_itemsets = transactions.flatMap(lambda transaction: [frozenset(combo) for combo in combinations(transaction, k)]).map(lambda x: (x, 1))\n",
        "    # Aggregate counts for each k-itemset by summing up their occurrences in the dataset\n",
        "    transaction_k_counts = transaction_k_itemsets.reduceByKey(lambda a, b: a + b)\n",
        "    # Convert candidate k-itemsets into an RDD, join it with the transaction counts, and extract the counts for valid candidates\n",
        "    candidate_counts = sc.parallelize(list(candidates_k)).map(lambda item: (item, 1)).join(transaction_k_counts).map(lambda x: (x[0], x[1][1]))\n",
        "    # Retain only candidate k-itemsets whose counts meet or exceed the frequency threshold and collect them into a list\n",
        "    frequent_k_itemsets = candidate_counts.filter(lambda x: x[1] >= frequency_threshold).collect()\n",
        "\n",
        "    if not frequent_k_itemsets:\n",
        "        break\n",
        "\n",
        "    frequent_itemsets[k] = set()\n",
        "    for itemset, count in frequent_k_itemsets:\n",
        "        frequent_itemsets[k].add(itemset)\n",
        "        s_count[itemset] = count\n",
        "\n",
        "    print(f\"k={k}: {frequent_itemsets[k]}\")\n",
        "\n",
        "    k += 1"
      ],
      "metadata": {
        "id": "lzhOlwYMLzec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b3d57b-1fdb-4d13-9391-ed72859fbc97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous freq itemsets: {frozenset({'5'}), frozenset({'2'}), frozenset({'4'}), frozenset({'1'}), frozenset({'3'})}\n",
            "\n",
            "1 freq itemsets: {frozenset({'5'}), frozenset({'2'}), frozenset({'4'}), frozenset({'1'}), frozenset({'3'})}\n",
            "\n",
            "Generate candidates for k=2\n",
            "{frozenset({'3', '1'}), frozenset({'3', '4'}), frozenset({'3', '2'}), frozenset({'1', '4'}), frozenset({'1', '5'}), frozenset({'3', '5'}), frozenset({'2', '4'}), frozenset({'5', '4'}), frozenset({'5', '2'}), frozenset({'1', '2'})}\n",
            "\n",
            "Pruned candidates: {frozenset({'3', '1'}), frozenset({'5', '2'}), frozenset({'1', '4'}), frozenset({'1', '5'}), frozenset({'3', '5'}), frozenset({'2', '4'}), frozenset({'5', '4'}), frozenset({'3', '2'}), frozenset({'3', '4'}), frozenset({'1', '2'})}\n",
            "k=2: {frozenset({'1', '4'}), frozenset({'3', '2'}), frozenset({'1', '5'}), frozenset({'2', '4'}), frozenset({'5', '4'}), frozenset({'3', '4'}), frozenset({'1', '2'})}\n",
            "Previous freq itemsets: {frozenset({'1', '4'}), frozenset({'3', '2'}), frozenset({'1', '5'}), frozenset({'2', '4'}), frozenset({'5', '4'}), frozenset({'3', '4'}), frozenset({'1', '2'})}\n",
            "\n",
            "1 freq itemsets: {frozenset({'5'}), frozenset({'2'}), frozenset({'4'}), frozenset({'1'}), frozenset({'3'})}\n",
            "\n",
            "Generate candidates for k=3\n",
            "{frozenset({'3', '1', '4'}), frozenset({'1', '5', '2'}), frozenset({'3', '1', '2'}), frozenset({'2', '1', '4'}), frozenset({'3', '1', '5'}), frozenset({'2', '5', '4'}), frozenset({'3', '5', '4'}), frozenset({'3', '2', '4'}), frozenset({'3', '5', '2'}), frozenset({'1', '5', '4'})}\n",
            "\n",
            "Pruned candidates: {frozenset({'2', '1', '4'}), frozenset({'1', '5', '4'}), frozenset({'3', '2', '4'})}\n",
            "k=3: {frozenset({'3', '2', '4'})}\n",
            "Previous freq itemsets: {frozenset({'3', '2', '4'})}\n",
            "\n",
            "1 freq itemsets: {frozenset({'5'}), frozenset({'2'}), frozenset({'4'}), frozenset({'1'}), frozenset({'3'})}\n",
            "\n",
            "Generate candidates for k=4\n",
            "{frozenset({'3', '2', '1', '4'}), frozenset({'3', '2', '5', '4'})}\n",
            "\n",
            "Pruned candidates: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_frequent_itemsets = []\n",
        "for size, itemsets in frequent_itemsets.items():\n",
        "    for itemset in itemsets:\n",
        "        all_frequent_itemsets.append((frozenset(itemset), size, s_count[itemset]))\n",
        "\n",
        "all_frequent_itemsets = sorted(all_frequent_itemsets, key=lambda x: (x[1], sorted(x[0])))\n",
        "\n",
        "end_time_frequent_itemsets = time.time()\n",
        "\n",
        "print(\"\\nAll Frequent Itemsets:\")\n",
        "for itemset, size, count in all_frequent_itemsets:\n",
        "    print(f\"Itemset: {set(itemset)}, Size: {size}, Support Count: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oDs8apZja32",
        "outputId": "8c4fe059-1da8-47cd-9224-38b8e0d49b51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All Frequent Itemsets:\n",
            "Itemset: {'1'}, Size: 1, Support Count: 10\n",
            "Itemset: {'2'}, Size: 1, Support Count: 8\n",
            "Itemset: {'3'}, Size: 1, Support Count: 4\n",
            "Itemset: {'4'}, Size: 1, Support Count: 5\n",
            "Itemset: {'5'}, Size: 1, Support Count: 5\n",
            "Itemset: {'1', '2'}, Size: 2, Support Count: 5\n",
            "Itemset: {'1', '4'}, Size: 2, Support Count: 3\n",
            "Itemset: {'1', '5'}, Size: 2, Support Count: 3\n",
            "Itemset: {'3', '2'}, Size: 2, Support Count: 3\n",
            "Itemset: {'2', '4'}, Size: 2, Support Count: 3\n",
            "Itemset: {'3', '4'}, Size: 2, Support Count: 3\n",
            "Itemset: {'5', '4'}, Size: 2, Support Count: 3\n",
            "Itemset: {'3', '2', '4'}, Size: 3, Support Count: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execution_time_frequent_itemsets = end_time_frequent_itemsets - start_time_frequent_itemsets\n",
        "print(f\"Time to find frequent itemsets: {execution_time_frequent_itemsets:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwOdhWSgc_kB",
        "outputId": "ee624651-e50d-41fe-c4ce-a19dc19a1e78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to find frequent itemsets: 10.1350 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Association Rules"
      ],
      "metadata": {
        "id": "zsLFulQBCCYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Association rules are relationships between items in a dataset, where the presence of certain items (antecedents) suggests the likelihood of the presence of other items (consequents). This is how association rules are generated:\n",
        "\n",
        "1) Find frequent itemsets\n",
        "\n",
        "2) For each frequent itemset, find its subsets (antecedents)\n",
        "\n",
        "3) For each subset, create a rule where the subset is the antecedent, and the remaining items are the consequent\n",
        "\n",
        "4) Calculate the confidence of the rule and keep only those rules that meet the confidence threshold (where confidence (A --> B) = Support(A U B) / Support(A))"
      ],
      "metadata": {
        "id": "DpmeBaMWCEqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_association_rules = time.time()\n",
        "\n",
        "association_rules = []\n",
        "\n",
        "# Loop through all frequent itemsets\n",
        "for itemset, size, count in all_frequent_itemsets:\n",
        "    if size < 2: # Rules require at least one antecedent and one consequent\n",
        "        continue\n",
        "\n",
        "    # Generate all non-empty proper subsets (antecedents) of the itemset\n",
        "    subsets = []\n",
        "    for r in range(1, size):\n",
        "        subsets.extend(combinations(itemset, r)) # Generate combinations of size r\n",
        "\n",
        "    # Process each subset to form rules\n",
        "    for subset in subsets:\n",
        "        antecedent = frozenset(subset)\n",
        "        consequent = itemset - antecedent # Calculate the consequent by subtracting antecedent from the itemset\n",
        "\n",
        "        if not consequent: # Ensure the consequent is not empty\n",
        "            continue\n",
        "\n",
        "        # Retrieve support counts\n",
        "        support_itemset = int(count) # Support count of the full itemset\n",
        "        support_antecedent = int(s_count.get(antecedent, 0)) # Support count of the antecedent\n",
        "\n",
        "        if support_antecedent == 0:\n",
        "            continue  # Avoid division by zero\n",
        "\n",
        "        # Calculate confidence: P(Consequent | Antecedent) = Support(Itemset) / Support(Antecedent)\n",
        "        confidence = support_itemset / support_antecedent\n",
        "\n",
        "        if confidence >= c:\n",
        "            # Append the rule with its metrics to the list\n",
        "            association_rules.append((set(antecedent), set(consequent), support_itemset, support_antecedent, confidence))\n"
      ],
      "metadata": {
        "id": "hq44YXK8Xejy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort association rules by confidence in descending order\n",
        "association_rules = sorted(association_rules, key=lambda x: x[4], reverse=True)\n",
        "\n",
        "end_time_association_rules = time.time()\n",
        "\n",
        "for antecedent, consequent, support_S, support_A, confidence in association_rules:\n",
        "    print(f\"Rule: {antecedent} -> {consequent}, Support(A U B): {support_S}, Support(A): {support_A}, Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYIvcot8Y6NL",
        "outputId": "97addba4-5c35-4af0-9869-1f1c984711d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rule: {'3', '2'} -> {'4'}, Support(A U B): 3, Support(A): 3, Confidence: 1.00\n",
            "Rule: {'3', '4'} -> {'2'}, Support(A U B): 3, Support(A): 3, Confidence: 1.00\n",
            "Rule: {'2', '4'} -> {'3'}, Support(A U B): 3, Support(A): 3, Confidence: 1.00\n",
            "Rule: {'3'} -> {'2'}, Support(A U B): 3, Support(A): 4, Confidence: 0.75\n",
            "Rule: {'3'} -> {'4'}, Support(A U B): 3, Support(A): 4, Confidence: 0.75\n",
            "Rule: {'3'} -> {'2', '4'}, Support(A U B): 3, Support(A): 4, Confidence: 0.75\n",
            "Rule: {'2'} -> {'1'}, Support(A U B): 5, Support(A): 8, Confidence: 0.62\n",
            "Rule: {'4'} -> {'1'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'5'} -> {'1'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'4'} -> {'2'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'4'} -> {'3'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'5'} -> {'4'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'4'} -> {'5'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n",
            "Rule: {'4'} -> {'3', '2'}, Support(A U B): 3, Support(A): 5, Confidence: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execution_time_association_rules = end_time_association_rules - start_time_association_rules\n",
        "print(f\"Time to generate association rules: {execution_time_association_rules:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSIDbJ-BdPkN",
        "outputId": "c2539dca-99f4-42d4-9bac-a956947c8f4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to generate association rules: 0.0104 seconds\n"
          ]
        }
      ]
    }
  ]
}