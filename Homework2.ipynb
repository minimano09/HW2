{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2\n",
    "\n",
    "### Mandatory Task:\n",
    "You are to solve the first sub-problem: to implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions. Remind that support of an itemset is the number of transactions containing the itemset. To test and evaluate your implementation, write a program that uses your A-Priori algorithm implementation to discover frequent itemsets with support at least s in a given dataset of sales transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/14 20:16:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initializing Spark\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(\"FreqItemSets\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions = sc.textFile(\"datasets/transaction_dataset2.txt\").map(lambda line: line.strip().split(\" \"))\n",
    "print(transactions.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6\n"
     ]
    }
   ],
   "source": [
    "# Parameters:\n",
    "s = 0.2 # support threshold - 0.018 there is 3-itemsets as well, but running time is too much for me\n",
    "num_of_transactions = transactions.count()\n",
    "frequency_threshold = s * num_of_transactions # how many times should the itemset apper to be frequent\n",
    "c = 0.6\n",
    "print(frequency_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(prev_freq_itemsets, freq_1_itemsets, k):\n",
    "    \"\"\"Generate candidate k-itemsets by pairing (k-1)-itemsets with 1-itemsets.\"\"\"\n",
    "    print(f\"Prev freq itemsets: {prev_freq_itemsets}\")\n",
    "    print(f\"1 freq itemsets: {freq_1_itemsets}\")\n",
    "    print(f\"generate candidates for k={k}\" )\n",
    "    candidates = set()\n",
    "    \n",
    "    for itemset in prev_freq_itemsets:\n",
    "        for item in freq_1_itemsets:\n",
    "            # Create a new candidate by adding the 1-itemset to the (k-1)-itemset\n",
    "            candidate = itemset | item\n",
    "            \n",
    "            # Only add if the resulting candidate has exactly k items\n",
    "            if len(candidate) == k:\n",
    "                candidates.add(candidate)\n",
    "                \n",
    "    print(candidates)\n",
    "    \n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_candidates(candidates, prev_freq_itemsets):\n",
    "    \"\"\"Prune candidate k-itemsets by removing those with infrequent (k-1)-itemset subsets.\"\"\"\n",
    "    pruned_candidates = set()\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        is_valid = True\n",
    "        # Generate all possible (k-1)-itemsets by removing one item at a time\n",
    "        for item in candidate:\n",
    "            subset = candidate - frozenset([item])\n",
    "            # Check if the subset is in the frequent (k-1)-itemsets\n",
    "            if subset not in prev_freq_itemsets:\n",
    "                is_valid = False\n",
    "                break\n",
    "        # If all (k-1)-subsets are frequent, add candidate to the pruned set\n",
    "        if is_valid:\n",
    "            pruned_candidates.add(candidate)\n",
    "            \n",
    "    print(f\"Pruned candidates: {pruned_candidates}\")\n",
    "    \n",
    "    return pruned_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{frozenset({'1'}): 10, frozenset({'4'}): 5, frozenset({'2'}): 8, frozenset({'3'}): 4, frozenset({'5'}): 5}\n"
     ]
    }
   ],
   "source": [
    "freq_itemsets = dict()\n",
    "s_count = dict()\n",
    "\n",
    "item_appear = transactions.flatMap(lambda items: [(item, 1) for item in items]).reduceByKey(lambda x, y: x+y)\n",
    "freq_1_itemsets_tuples = item_appear.filter(lambda item: item[1] >= frequency_threshold).map(lambda item: (frozenset([item[0]]), item[1])).collect()\n",
    "\n",
    "freq_1_itemsets = set()\n",
    "\n",
    "for itemset, count in freq_1_itemsets_tuples:\n",
    "    freq_itemsets.setdefault(1, set()).add(itemset)\n",
    "    s_count[itemset] = count\n",
    "    freq_1_itemsets.update(itemset)\n",
    "\n",
    "print(len(freq_1_itemsets))\n",
    "print(s_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_1_itemsets: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}\n"
     ]
    }
   ],
   "source": [
    "# Convert individual items in freq_1_itemsets to frozensets\n",
    "freq_1_itemsets = set(frozenset([item]) for item in freq_1_itemsets)\n",
    "# Collect frequent 1-itemsets as frozensets of individual items\n",
    "#freq_1_itemsets = set(item for item, count in freq_1_itemsets_tuples if count >= frequency_threshold)\n",
    "# Extract individual string items from frozenset\n",
    "#freq_1_itemsets = set(next(iter(itemset)) for itemset, count in freq_1_itemsets_tuples if count >= frequency_threshold)\n",
    "print(f\"freq_1_itemsets: {freq_1_itemsets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev freq itemsets: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}\n",
      "1 freq itemsets: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}\n",
      "generate candidates for k=2\n",
      "{frozenset({'5', '1'}), frozenset({'5', '3'}), frozenset({'3', '1'}), frozenset({'5', '4'}), frozenset({'5', '2'}), frozenset({'2', '4'}), frozenset({'2', '1'}), frozenset({'2', '3'}), frozenset({'1', '4'}), frozenset({'3', '4'})}\n",
      "Pruned candidates: {frozenset({'5', '1'}), frozenset({'5', '3'}), frozenset({'3', '1'}), frozenset({'5', '4'}), frozenset({'5', '2'}), frozenset({'2', '4'}), frozenset({'2', '1'}), frozenset({'2', '3'}), frozenset({'1', '4'}), frozenset({'3', '4'})}\n",
      "k=2: {frozenset({'5', '1'}), frozenset({'5', '4'}), frozenset({'2', '4'}), frozenset({'2', '1'}), frozenset({'2', '3'}), frozenset({'1', '4'}), frozenset({'3', '4'})}\n",
      "Prev freq itemsets: {frozenset({'5', '1'}), frozenset({'5', '4'}), frozenset({'2', '4'}), frozenset({'2', '1'}), frozenset({'2', '3'}), frozenset({'1', '4'}), frozenset({'3', '4'})}\n",
      "1 freq itemsets: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}\n",
      "generate candidates for k=3\n",
      "{frozenset({'1', '4', '3'}), frozenset({'2', '1', '3'}), frozenset({'5', '1', '3'}), frozenset({'5', '4', '2'}), frozenset({'5', '1', '2'}), frozenset({'2', '1', '4'}), frozenset({'2', '3', '5'}), frozenset({'5', '1', '4'}), frozenset({'2', '3', '4'}), frozenset({'5', '3', '4'})}\n",
      "Pruned candidates: {frozenset({'2', '1', '4'}), frozenset({'5', '1', '4'}), frozenset({'2', '3', '4'})}\n",
      "k=3: {frozenset({'2', '3', '4'})}\n",
      "Prev freq itemsets: {frozenset({'2', '3', '4'})}\n",
      "1 freq itemsets: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}\n",
      "generate candidates for k=4\n",
      "{frozenset({'2', '3', '4', '5'}), frozenset({'2', '3', '4', '1'})}\n",
      "Pruned candidates: set()\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "\n",
    "while True:\n",
    "    candidates_k = generate_candidates(freq_itemsets[k-1], freq_1_itemsets, k)\n",
    "    if not candidates_k:\n",
    "        break\n",
    "    \n",
    "    #Pruning:\n",
    "    candidates_k = prune_candidates(candidates_k, freq_itemsets[k-1])\n",
    "    if not candidates_k:\n",
    "        break\n",
    "    \n",
    "    candidates_k_rdd = sc.parallelize(list(candidates_k)).map(lambda item: (item, 1)) # every element is in (itemset, 1) format\n",
    "    \n",
    "    transaction_k_itemsets = transactions.flatMap(lambda transaction: [frozenset(combo) for combo in combinations(transaction, k)]).map(lambda x: (x, 1)) # contains the subsets as well from candidates_k_rdd in the format (itemset, 1)\n",
    "        \n",
    "    # Join candidate itemsets with transaction itemsets to count support\n",
    "    #candidate_counts = candidates_k_rdd.join(transaction_k_itemsets).map(lambda x: (x[0], x[1][0] + x[1][1])).reduceByKey(lambda a, b: a + b)\n",
    "    # Join candidate itemsets with transaction itemsets to count support\n",
    "    #candidate_counts = candidates_k_rdd.join(transaction_k_itemsets).map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    \n",
    "    #freq_k_itemsets = candidate_counts.filter(lambda x: x[1] >= frequency_threshold).collect()\n",
    "    \n",
    "    transaction_k_counts = transaction_k_itemsets.reduceByKey(lambda a, b: a + b)\n",
    "    candidate_counts = candidates_k_rdd.join(transaction_k_counts).map(lambda x: (x[0], x[1][1]))\n",
    "    freq_k_itemsets = candidate_counts.filter(lambda x: x[1] >= frequency_threshold).collect()\n",
    "\n",
    "        \n",
    "    if not freq_k_itemsets:\n",
    "        break\n",
    "\n",
    "    freq_itemsets[k] = set()\n",
    "    for itemset, count in freq_k_itemsets:\n",
    "        freq_itemsets[k].add(itemset)\n",
    "        s_count[itemset] = count\n",
    "    print(f\"k={k}: {freq_itemsets[k]}\")\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {frozenset({'5'}), frozenset({'3'}), frozenset({'1'}), frozenset({'2'}), frozenset({'4'})}, 2: {frozenset({'5', '1'}), frozenset({'5', '4'}), frozenset({'2', '4'}), frozenset({'2', '1'}), frozenset({'2', '3'}), frozenset({'1', '4'}), frozenset({'3', '4'})}, 3: {frozenset({'2', '3', '4'})}}\n"
     ]
    }
   ],
   "source": [
    "print(freq_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequent_itemsets = []\n",
    "for size, itemsets in freq_itemsets.items():\n",
    "    for itemset in itemsets:\n",
    "        all_frequent_itemsets.append((frozenset(itemset), size, s_count[itemset]))\n",
    "\n",
    "all_frequent_itemsets = sorted(all_frequent_itemsets, key=lambda x: (x[1], sorted(x[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Frequent Itemsets:\n",
      "Itemset: {'1'}, Size: 1, Support Count: 10\n",
      "Itemset: {'2'}, Size: 1, Support Count: 8\n",
      "Itemset: {'3'}, Size: 1, Support Count: 4\n",
      "Itemset: {'4'}, Size: 1, Support Count: 5\n",
      "Itemset: {'5'}, Size: 1, Support Count: 5\n",
      "Itemset: {'2', '1'}, Size: 2, Support Count: 5\n",
      "Itemset: {'1', '4'}, Size: 2, Support Count: 3\n",
      "Itemset: {'5', '1'}, Size: 2, Support Count: 3\n",
      "Itemset: {'2', '3'}, Size: 2, Support Count: 3\n",
      "Itemset: {'2', '4'}, Size: 2, Support Count: 3\n",
      "Itemset: {'3', '4'}, Size: 2, Support Count: 3\n",
      "Itemset: {'5', '4'}, Size: 2, Support Count: 3\n",
      "Itemset: {'2', '3', '4'}, Size: 3, Support Count: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAll Frequent Itemsets:\")\n",
    "for itemset, size, count in all_frequent_itemsets:\n",
    "    print(f\"Itemset: {set(itemset)}, Size: {size}, Support Count: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = []\n",
    "\n",
    "for itemset, size, count in all_frequent_itemsets:\n",
    "    if size < 2:\n",
    "        continue\n",
    "    # Generate all non-empty proper subsets of the itemset\n",
    "    subsets = []\n",
    "    for r in range(1, size):\n",
    "        subsets.extend(combinations(itemset, r))\n",
    "    \n",
    "    # Process each subset to form rules\n",
    "    for subset in subsets:\n",
    "        antecedent = frozenset(subset)\n",
    "        consequent = itemset - antecedent\n",
    "        if not consequent:\n",
    "            continue\n",
    "        # Retrieve support counts\n",
    "        support_itemset = int(count)\n",
    "        support_antecedent = int(s_count.get(antecedent, 0))\n",
    "        if support_antecedent == 0:\n",
    "            continue  # Avoid division by zero\n",
    "        confidence = support_itemset / support_antecedent\n",
    "        if confidence >= c:\n",
    "            association_rules.append((set(antecedent), set(consequent), support_itemset, support_antecedent, confidence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: {'2', '3'} -> {'4'}, Support(L): 3, Support(A): 3, Confidence: 1.00\n",
      "Rule: {'2', '4'} -> {'3'}, Support(L): 3, Support(A): 3, Confidence: 1.00\n",
      "Rule: {'3', '4'} -> {'2'}, Support(L): 3, Support(A): 3, Confidence: 1.00\n",
      "Rule: {'3'} -> {'2'}, Support(L): 3, Support(A): 4, Confidence: 0.75\n",
      "Rule: {'3'} -> {'4'}, Support(L): 3, Support(A): 4, Confidence: 0.75\n",
      "Rule: {'3'} -> {'2', '4'}, Support(L): 3, Support(A): 4, Confidence: 0.75\n",
      "Rule: {'2'} -> {'1'}, Support(L): 5, Support(A): 8, Confidence: 0.62\n",
      "Rule: {'4'} -> {'1'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'5'} -> {'1'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'4'} -> {'2'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'4'} -> {'3'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'5'} -> {'4'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'4'} -> {'5'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n",
      "Rule: {'4'} -> {'2', '3'}, Support(L): 3, Support(A): 5, Confidence: 0.60\n"
     ]
    }
   ],
   "source": [
    "association_rules = sorted(association_rules, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "for antecedent, consequent, support_S, support_A, confidence in association_rules:\n",
    "    print(f\"Rule: {antecedent} -> {consequent}, Support(L): {support_S}, Support(A): {support_A}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
